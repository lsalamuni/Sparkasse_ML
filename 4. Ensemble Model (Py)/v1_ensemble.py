#!/usr/bin/env python3
"""
Enhanced XGBoost Email Classifier v1 - German Banking Emails
Improvements:
- Fixed XGBoost API compatibility issues
- Better hyperparameter optimization
- Feature selection to reduce overfitting
- Ensemble approach with voting
- SMOTE for handling class imbalance
- Better cross-validation strategy
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import (
    train_test_split, StratifiedKFold, GridSearchCV,
    cross_val_score, cross_validate
)
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import (
    classification_report, confusion_matrix, f1_score,
    accuracy_score, precision_recall_fscore_support,
    balanced_accuracy_score, cohen_kappa_score
)
from sklearn.utils.class_weight import compute_sample_weight, compute_class_weight
from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif
from sklearn.ensemble import VotingClassifier
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
import xgboost as xgb
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
import joblib
warnings.filterwarnings('ignore')


class EnhancedXGBoostClassifier:
    """Enhanced XGBoost classifier with better accuracy and error handling"""
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.label_encoder = LabelEncoder()
        self.scaler = StandardScaler()
        self.feature_selector = None
        self.model = None
        self.ensemble_model = None
        self.best_params = None
        self.selected_features = None
        self.feature_importance_df = None
        
    def load_data(self, filepath='ml_dataset_v5_full.xlsx'):
        """Load and prepare data with validation"""
        try:
            print(f"Loading data from {filepath}...")
            self.df = pd.read_excel(filepath)
            
            # Data validation
            if self.df.empty:
                raise ValueError("Dataset is empty")
            
            # Separate features and target
            self.X = self.df.drop(['document', 'category'], axis=1)
            self.y = self.df['category']
            
            # Handle missing values if any
            if self.X.isnull().any().any():
                print("Warning: Missing values detected. Filling with median...")
                self.X = self.X.fillna(self.X.median())
            
            # Remove constant features
            constant_features = self.X.columns[self.X.nunique() == 1]
            if len(constant_features) > 0:
                print(f"Removing {len(constant_features)} constant features...")
                self.X = self.X.drop(columns=constant_features)
            
            # Encode categories
            self.y_encoded = self.label_encoder.fit_transform(self.y)
            self.n_classes = len(np.unique(self.y_encoded))
            self.class_names = self.label_encoder.classes_
            
            print(f"Dataset shape: {self.X.shape}")
            print(f"Number of classes: {self.n_classes}")
            print(f"Features after preprocessing: {self.X.shape[1]}")
            
            # Class distribution analysis
            self._analyze_class_distribution()
            
        except Exception as e:
            print(f"Error loading data: {str(e)}")
            raise
        
    def _analyze_class_distribution(self):
        """Enhanced class distribution analysis"""
        class_counts = pd.Series(self.y).value_counts()
        
        print("\nClass Distribution:")
        for cat, count in class_counts.items():
            print(f"{cat}: {count} ({count/len(self.y)*100:.1f}%)")
        
        # Calculate class weights for later use
        self.class_weights = compute_class_weight(
            'balanced',
            classes=np.unique(self.y_encoded),
            y=self.y_encoded
        )
        self.class_weight_dict = dict(enumerate(self.class_weights))
        
        # Imbalance metrics
        max_class = class_counts.max()
        min_class = class_counts.min()
        imbalance_ratio = max_class / min_class
        
        print(f"\nImbalance Ratio: {imbalance_ratio:.2f}")
        print(f"Largest class: {max_class} samples")
        print(f"Smallest class: {min_class} samples")
        
        # Visualize distribution
        plt.figure(figsize=(10, 6))
        class_counts.plot(kind='bar', color='skyblue', edgecolor='black')
        plt.title('Class Distribution in Dataset')
        plt.xlabel('Category')
        plt.ylabel('Number of Samples')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.show()
        
    def feature_selection(self, n_features='auto', method='mutual_info'):
        """Select most important features to reduce overfitting"""
        print("\nPerforming feature selection...")
        
        if n_features == 'auto':
            # Rule of thumb: sqrt(n_samples) * 2
            n_features = min(int(np.sqrt(len(self.X_train)) * 2), self.X_train.shape[1])
        
        if method == 'mutual_info':
            selector = SelectKBest(mutual_info_classif, k=n_features)
        else:
            selector = SelectKBest(chi2, k=n_features)
        
        # Fit selector
        self.feature_selector = selector
        X_train_selected = selector.fit_transform(self.X_train_scaled, self.y_train)
        
        # Get selected feature names
        feature_scores = pd.DataFrame({
            'feature': self.X.columns,
            'score': selector.scores_
        }).sort_values('score', ascending=False)
        
        self.selected_features = self.X.columns[selector.get_support()].tolist()
        
        print(f"Selected {len(self.selected_features)} features from {self.X_train.shape[1]}")
        print(f"Top 10 features by importance:")
        print(feature_scores.head(10))
        
        return X_train_selected
        
    def split_data(self, test_size=0.2, val_size=0.15):
        """Enhanced data splitting with scaling"""
        # First split: train+val vs test
        X_temp, self.X_test, y_temp, self.y_test = train_test_split(
            self.X, self.y_encoded,
            test_size=test_size,
            stratify=self.y_encoded,
            random_state=self.random_state
        )
        
        # Second split: train vs val
        val_size_adjusted = val_size / (1 - test_size)
        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(
            X_temp, y_temp,
            test_size=val_size_adjusted,
            stratify=y_temp,
            random_state=self.random_state
        )
        
        # Scale features
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_val_scaled = self.scaler.transform(self.X_val)
        self.X_test_scaled = self.scaler.transform(self.X_test)
        
        import sys
        
        print(f"\nData splits:", flush=True)
        print(f"Train: {len(self.X_train)} samples ({len(self.X_train)/len(self.X)*100:.1f}%)", flush=True)
        print(f"Validation: {len(self.X_val)} samples ({len(self.X_val)/len(self.X)*100:.1f}%)", flush=True)
        print(f"Test: {len(self.X_test)} samples ({len(self.X_test)/len(self.X)*100:.1f}%)", flush=True)
        
        # Verify split sizes match v0 expectations
        expected_train = 161
        expected_val = 38
        expected_test = 50
        
        if len(self.X_train) != expected_train or len(self.X_val) != expected_val or len(self.X_test) != expected_test:
            print(f"\nNote: Split sizes differ from v0 due to random_state or stratification:", flush=True)
            print(f"  Expected: Train={expected_train}, Val={expected_val}, Test={expected_test}", flush=True)
            print(f"  Actual:   Train={len(self.X_train)}, Val={len(self.X_val)}, Test={len(self.X_test)}", flush=True)
        else:
            print(f"\nâœ“ Split sizes match v0 exactly!", flush=True)
        
        sys.stdout.flush()
        
        # Calculate sample weights
        self.sample_weights = compute_sample_weight('balanced', self.y_train)
        
    def hyperparameter_tuning_enhanced(self, cv_folds=5, use_smote=True):
        """Enhanced hyperparameter tuning with SMOTE option"""
        print("\nPerforming enhanced hyperparameter tuning...")
        
        # More focused parameter grid based on best practices
        param_grid = {
            'xgbclassifier__max_depth': [3, 4, 5, 6],
            'xgbclassifier__learning_rate': [0.01, 0.05, 0.1, 0.15],
            'xgbclassifier__n_estimators': [100, 150, 200, 300],
            'xgbclassifier__min_child_weight': [1, 2, 3],
            'xgbclassifier__gamma': [0, 0.1, 0.2],
            'xgbclassifier__subsample': [0.7, 0.8, 0.9],
            'xgbclassifier__colsample_bytree': [0.7, 0.8, 0.9],
            'xgbclassifier__reg_alpha': [0, 0.1, 0.5, 1],  # L1 regularization
            'xgbclassifier__reg_lambda': [1, 1.5, 2, 3]   # L2 regularization
        }
        
        # Create base model with fixed parameters
        base_model = xgb.XGBClassifier(
            objective='multi:softprob',
            n_jobs=-1,
            random_state=self.random_state,
            use_label_encoder=False,
            eval_metric='mlogloss',
            early_stopping_rounds=10,
            verbosity=0
        )
        
        if use_smote:
            # Create pipeline with SMOTE
            pipeline = ImbPipeline([
                ('smote', SMOTE(random_state=self.random_state, k_neighbors=3)),
                ('xgbclassifier', base_model)
            ])
        else:
            pipeline = ImbPipeline([
                ('xgbclassifier', base_model)
            ])
        
        # Stratified K-Fold
        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state)
        
        # Use RandomizedSearchCV for faster tuning
        from sklearn.model_selection import RandomizedSearchCV
        
        random_search = RandomizedSearchCV(
            pipeline,
            param_grid,
            n_iter=50,  # Number of parameter combinations to try
            cv=skf,
            scoring='f1_macro',
            n_jobs=-1,
            verbose=1,
            random_state=self.random_state
        )
        
        # Fit with feature selection if enabled
        if self.feature_selector:
            X_train_selected = self.feature_selector.transform(self.X_train_scaled)
            random_search.fit(X_train_selected, self.y_train)
        else:
            random_search.fit(self.X_train_scaled, self.y_train)
        
        self.best_params = random_search.best_params_
        print(f"\nBest parameters found:")
        for param, value in self.best_params.items():
            print(f"  {param}: {value}")
        print(f"\nBest CV Macro-F1: {random_search.best_score_:.3f}")
        
        return random_search.best_estimator_
    
    def train_ensemble_model(self):
        """Train ensemble of models for better accuracy"""
        print("\nTraining ensemble model...")
        
        # Extract XGBoost parameters from best_params
        xgb_params = {k.replace('xgbclassifier__', ''): v 
                      for k, v in self.best_params.items() 
                      if 'xgbclassifier__' in k}
        
        # Model 1: XGBoost with best parameters
        xgb1 = xgb.XGBClassifier(
            **xgb_params,
            objective='multi:softprob',
            n_jobs=-1,
            random_state=self.random_state,
            use_label_encoder=False,
            eval_metric='mlogloss'
        )
        
        # Model 2: XGBoost with different seed and slightly different params
        xgb2 = xgb.XGBClassifier(
            **{**xgb_params, 
               'max_depth': xgb_params.get('max_depth', 5) + 1,
               'n_estimators': xgb_params.get('n_estimators', 200) + 50},
            objective='multi:softprob',
            n_jobs=-1,
            random_state=self.random_state + 42,
            use_label_encoder=False,
            eval_metric='mlogloss'
        )
        
        # Model 3: XGBoost with focus on reducing overfitting
        xgb3 = xgb.XGBClassifier(
            **{**xgb_params,
               'max_depth': max(3, xgb_params.get('max_depth', 5) - 1),
               'reg_alpha': xgb_params.get('reg_alpha', 0) + 0.5,
               'reg_lambda': xgb_params.get('reg_lambda', 1) + 0.5},
            objective='multi:softprob',
            n_jobs=-1,
            random_state=self.random_state + 84,
            use_label_encoder=False,
            eval_metric='mlogloss'
        )
        
        # Create voting ensemble
        self.ensemble_model = VotingClassifier(
            estimators=[
                ('xgb1', xgb1),
                ('xgb2', xgb2),
                ('xgb3', xgb3)
            ],
            voting='soft',
            n_jobs=-1
        )
        
        # Train ensemble
        if self.feature_selector:
            X_train = self.feature_selector.transform(self.X_train_scaled)
        else:
            X_train = self.X_train_scaled
            
        self.ensemble_model.fit(X_train, self.y_train)
        print("Ensemble training completed!")
        
    def train_single_model(self):
        """Train single XGBoost model with best parameters"""
        print("\nTraining optimized XGBoost model...")
        
        # Extract XGBoost parameters
        if self.best_params:
            xgb_params = {k.replace('xgbclassifier__', ''): v 
                          for k, v in self.best_params.items() 
                          if 'xgbclassifier__' in k}
        else:
            xgb_params = {
                'max_depth': 5,
                'learning_rate': 0.1,
                'n_estimators': 200,
                'min_child_weight': 2,
                'gamma': 0.1,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'reg_alpha': 0.5,
                'reg_lambda': 2
            }
        
        self.model = xgb.XGBClassifier(
            **xgb_params,
            objective='multi:softprob',
            n_jobs=-1,
            random_state=self.random_state,
            use_label_encoder=False,
            eval_metric='mlogloss'
        )
        
        # Prepare data
        if self.feature_selector:
            X_train = self.feature_selector.transform(self.X_train_scaled)
            X_val = self.feature_selector.transform(self.X_val_scaled)
        else:
            X_train = self.X_train_scaled
            X_val = self.X_val_scaled
        
        # Train with evaluation set
        self.model.fit(
            X_train, self.y_train,
            sample_weight=self.sample_weights,
            eval_set=[(X_val, self.y_val)],
            verbose=False
        )
        
        print("Model training completed!")
        
    def evaluate_model(self, use_ensemble=False):
        """Comprehensive model evaluation"""
        print("\nEvaluating model performance...")
        
        model = self.ensemble_model if use_ensemble else self.model
        
        # Prepare data
        if self.feature_selector:
            X_train = self.feature_selector.transform(self.X_train_scaled)
            X_val = self.feature_selector.transform(self.X_val_scaled)
            X_test = self.feature_selector.transform(self.X_test_scaled)
        else:
            X_train = self.X_train_scaled
            X_val = self.X_val_scaled
            X_test = self.X_test_scaled
        
        # Predictions
        y_pred_train = model.predict(X_train)
        y_pred_val = model.predict(X_val)
        y_pred_test = model.predict(X_test)
        
        # Calculate metrics
        results = {}
        for name, y_true, y_pred, X_data in [
            ('Train', self.y_train, y_pred_train, X_train),
            ('Validation', self.y_val, y_pred_val, X_val),
            ('Test', self.y_test, y_pred_test, X_test)
        ]:
            accuracy = accuracy_score(y_true, y_pred)
            balanced_acc = balanced_accuracy_score(y_true, y_pred)
            macro_f1 = f1_score(y_true, y_pred, average='macro')
            weighted_f1 = f1_score(y_true, y_pred, average='weighted')
            kappa = cohen_kappa_score(y_true, y_pred)
            
            # Per-class metrics
            precision, recall, f1, support = precision_recall_fscore_support(
                y_true, y_pred, average=None
            )
            
            results[name] = {
                'accuracy': accuracy,
                'balanced_accuracy': balanced_acc,
                'macro_f1': macro_f1,
                'weighted_f1': weighted_f1,
                'kappa': kappa,
                'y_true': y_true,
                'y_pred': y_pred,
                'per_class_f1': f1,
                'per_class_precision': precision,
                'per_class_recall': recall
            }
            
            print(f"\n{name} Set Performance:")
            print(f"  Accuracy: {accuracy:.3f}")
            print(f"  Balanced Accuracy: {balanced_acc:.3f}")
            print(f"  Macro F1: {macro_f1:.3f}")
            print(f"  Weighted F1: {weighted_f1:.3f}")
            print(f"  Cohen's Kappa: {kappa:.3f}")
        
        # Detailed test set report
        print("\nDetailed Test Set Classification Report:")
        print(classification_report(
            self.y_test, y_pred_test,
            target_names=self.class_names,
            digits=3
        ))
        
        # Per-class performance analysis
        self._analyze_per_class_performance(results['Test'])
        
        return results
    
    def _analyze_per_class_performance(self, test_results):
        """Analyze performance for each class"""
        per_class_df = pd.DataFrame({
            'Class': self.class_names,
            'F1-Score': test_results['per_class_f1'],
            'Precision': test_results['per_class_precision'],
            'Recall': test_results['per_class_recall']
        }).sort_values('F1-Score', ascending=False)
        
        print("\nPer-Class Performance (sorted by F1-Score):")
        print(per_class_df.to_string(index=False))
        
        # Identify problematic classes
        poor_classes = per_class_df[per_class_df['F1-Score'] < 0.5]
        if not poor_classes.empty:
            print(f"\nWarning: {len(poor_classes)} classes have F1-Score < 0.5:")
            print(poor_classes['Class'].tolist())
    
    def plot_confusion_matrix(self, y_true, y_pred, title="Confusion Matrix", normalize=True):
        """Enhanced confusion matrix visualization"""
        cm = confusion_matrix(y_true, y_pred)
        
        if normalize:
            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            fmt = '.2f'
        else:
            fmt = 'd'
        
        plt.figure(figsize=(12, 10))
        sns.heatmap(
            cm, annot=True, fmt=fmt, cmap='Blues',
            xticklabels=self.class_names,
            yticklabels=self.class_names,
            cbar_kws={'label': 'Proportion' if normalize else 'Count'}
        )
        plt.title(title)
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.show()
    
    def plot_feature_importance(self, top_n=30):
        """Enhanced feature importance visualization"""
        if not hasattr(self.model, 'feature_importances_'):
            print("Model doesn't have feature importances")
            return None
        
        # Get feature names
        if self.feature_selector:
            feature_names = [self.X.columns[i] for i in self.feature_selector.get_support(indices=True)]
        else:
            feature_names = self.X.columns.tolist()
        
        importance = self.model.feature_importances_
        
        # Create dataframe
        self.feature_importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False)
        
        # Plot top features
        plt.figure(figsize=(10, 8))
        top_features = self.feature_importance_df.head(top_n)
        
        sns.barplot(
            data=top_features, 
            x='importance', 
            y='feature',
            palette='viridis'
        )
        plt.title(f'Top {top_n} Most Important Features')
        plt.xlabel('Feature Importance Score')
        plt.tight_layout()
        plt.show()
        
        return self.feature_importance_df
    
    def cross_validate_enhanced(self, cv_folds=5):
        """Enhanced cross-validation with multiple metrics"""
        print(f"\nPerforming enhanced {cv_folds}-fold cross-validation...")
        
        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state)
        
        # Multiple scoring metrics
        scoring = {
            'accuracy': 'accuracy',
            'balanced_accuracy': 'balanced_accuracy',
            'f1_macro': 'f1_macro',
            'f1_weighted': 'f1_weighted'
        }
        
        # Prepare data
        if self.feature_selector:
            X_cv = self.feature_selector.transform(self.X_train_scaled)
        else:
            X_cv = self.X_train_scaled
        
        cv_results = cross_validate(
            self.model,
            X_cv, self.y_train,
            cv=skf,
            scoring=scoring,
            n_jobs=-1,
            return_train_score=True
        )
        
        # Summary statistics
        print("\nCross-Validation Results:")
        for metric in scoring.keys():
            train_scores = cv_results[f'train_{metric}']
            test_scores = cv_results[f'test_{metric}']
            print(f"\n{metric}:")
            print(f"  Train: {train_scores.mean():.3f} (+/- {train_scores.std()*2:.3f})")
            print(f"  Test:  {test_scores.mean():.3f} (+/- {test_scores.std()*2:.3f})")
            print(f"  Gap:   {(train_scores.mean() - test_scores.mean()):.3f}")
        
        return cv_results
    
    def save_model_and_results(self, results, cv_results, use_ensemble=False):
        """Save model, results, and configuration"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save model
        model = self.ensemble_model if use_ensemble else self.model
        model_type = 'ensemble' if use_ensemble else 'single'
        joblib.dump(model, f'xgboost_{model_type}_model_{timestamp}.pkl')
        
        # Save feature selector if used
        if self.feature_selector:
            joblib.dump(self.feature_selector, f'feature_selector_{timestamp}.pkl')
        
        # Save scaler
        joblib.dump(self.scaler, f'scaler_{timestamp}.pkl')
        
        # Save label encoder
        joblib.dump(self.label_encoder, f'label_encoder_{timestamp}.pkl')
        
        # Save predictions
        test_predictions = pd.DataFrame({
            'document': self.df.iloc[self.X_test.index]['document'],
            'true_category': self.label_encoder.inverse_transform(self.y_test),
            'predicted_category': self.label_encoder.inverse_transform(results['Test']['y_pred']),
            'correct': self.y_test == results['Test']['y_pred']
        })
        test_predictions.to_csv(f'v1_xgboost_predictions_{timestamp}.csv', index=False)
        
        # Save comprehensive metrics
        metrics_data = []
        for dataset in ['Train', 'Validation', 'Test']:
            metrics_data.append({
                'Dataset': dataset,
                'Accuracy': results[dataset]['accuracy'],
                'Balanced_Accuracy': results[dataset]['balanced_accuracy'],
                'Macro_F1': results[dataset]['macro_f1'],
                'Weighted_F1': results[dataset]['weighted_f1'],
                'Kappa': results[dataset]['kappa']
            })
        
        metrics_df = pd.DataFrame(metrics_data)
        metrics_df.to_csv(f'v1_xgboost_metrics_{timestamp}.csv', index=False)
        
        # Save configuration
        config = {
            'timestamp': timestamp,
            'model_type': model_type,
            'n_features_selected': len(self.selected_features) if self.feature_selector else 'all',
            'best_params': self.best_params,
            'class_weights': self.class_weight_dict,
            'feature_importance': self.feature_importance_df.to_dict() if self.feature_importance_df is not None else None
        }
        
        import json
        with open(f'v1_xgboost_config_{timestamp}.json', 'w') as f:
            json.dump(config, f, indent=4, default=str)
        
        print(f"\nAll results saved with timestamp: {timestamp}")
        return test_predictions, metrics_df


def main():
    """Main execution function with error handling"""
    try:
        # Initialize classifier
        classifier = EnhancedXGBoostClassifier(random_state=42)
        
        # Load data
        classifier.load_data('ml_dataset_v5_full.xlsx')
        
        # Split data
        print("About to split data...")
        classifier.split_data(test_size=0.2, val_size=0.15)
        print("Data splitting completed!")
        
        # Feature selection (reduces overfitting)
        classifier.feature_selection(n_features='auto', method='mutual_info')
        
        # Hyperparameter tuning with SMOTE
        best_pipeline = classifier.hyperparameter_tuning_enhanced(cv_folds=5, use_smote=True)
        
        # Train single optimized model
        classifier.train_single_model()
        
        # Train ensemble for comparison
        classifier.train_ensemble_model()
        
        # Evaluate single model
        print("\n" + "="*60)
        print("SINGLE MODEL EVALUATION")
        print("="*60)
        results_single = classifier.evaluate_model(use_ensemble=False)
        
        # Evaluate ensemble model
        print("\n" + "="*60)
        print("ENSEMBLE MODEL EVALUATION")
        print("="*60)
        results_ensemble = classifier.evaluate_model(use_ensemble=True)
        
        # Choose best model based on test performance
        if results_ensemble['Test']['macro_f1'] > results_single['Test']['macro_f1']:
            print("\nEnsemble model performs better. Using ensemble for final results.")
            final_results = results_ensemble
            use_ensemble = True
        else:
            print("\nSingle model performs better. Using single model for final results.")
            final_results = results_single
            use_ensemble = False
        
        # Visualizations
        classifier.plot_confusion_matrix(
            final_results['Test']['y_true'], 
            final_results['Test']['y_pred'],
            title="Test Set Confusion Matrix (Normalized)",
            normalize=True
        )
        
        # Feature importance (only for single model)
        if not use_ensemble:
            importance_df = classifier.plot_feature_importance(top_n=30)
        
        # Cross-validation
        cv_results = classifier.cross_validate_enhanced(cv_folds=5)
        
        # Save everything
        test_predictions, metrics_summary = classifier.save_model_and_results(
            final_results, cv_results, use_ensemble=use_ensemble
        )
        
        print("\n" + "="*60)
        print("FINAL RESULTS SUMMARY")
        print("="*60)
        print(f"Model Type: {'Ensemble' if use_ensemble else 'Single'}")
        print(f"Test Accuracy: {final_results['Test']['accuracy']:.3f}")
        print(f"Test Balanced Accuracy: {final_results['Test']['balanced_accuracy']:.3f}")
        print(f"Test Macro-F1: {final_results['Test']['macro_f1']:.3f}")
        print(f"Test Weighted-F1: {final_results['Test']['weighted_f1']:.3f}")
        print(f"Test Cohen's Kappa: {final_results['Test']['kappa']:.3f}")
        
        # Identify improvements
        if 'v0_results' in locals():
            improvement = final_results['Test']['macro_f1'] - v0_results['Test']['macro_f1']
            print(f"\nImprovement over v0: +{improvement:.3f} Macro-F1")
        
    except Exception as e:
        print(f"\nError in main execution: {str(e)}")
        import traceback
        traceback.print_exc()
        return None
    
    return classifier, final_results


if __name__ == "__main__":
    classifier, results = main()